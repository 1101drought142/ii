{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 1: Линейная регрессия и факторный анализ\n",
    "\n",
    "## Введение\n",
    "\n",
    "Целью данной лабораторной работы является изучение основ линейной регрессии, построение простейших моделей регрессии, проведение обучения модели на реальных данных и оценка её качества.\n",
    "\n",
    "В рамках работы необходимо провести обучение модели линейной регрессии на датасете с Kaggle, выполнить предобработку данных, провести анализ мультиколлинеарности, построить регрессионные модели (линейную и гребневую), устранить мультиколлинеарность с помощью метода главных компонент (PCA) и сравнить результаты моделей, обученных на исходных данных и на главных компонентах.\n",
    "\n",
    "Основные задачи работы включают загрузку и подготовку данных, визуализацию распределения признаков, построение матрицы корреляций, расчет VIF-коэффициентов для выявления мультиколлинеарности, обучение моделей регрессии с оценкой качества через метрики RMSE, R² и MAPE, применение PCA для снижения размерности признаков и сравнение результатов моделей до и после применения PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Настройки\n",
    "DATA_PATH = 'data.csv'\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "RIDGE_ALPHA = 1.0\n",
    "CV_FOLDS = 5\n",
    "PCA_VARIANCE_THRESHOLD = 0.95\n",
    "FIG_SIZE = (12, 8)\n",
    "DPI = 100\n",
    "\n",
    "# Создание папки для графиков\n",
    "photos_dir = os.path.join('photos')\n",
    "os.makedirs(photos_dir, exist_ok=True)\n",
    "\n",
    "# Настройка визуализации\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = FIG_SIZE\n",
    "plt.rcParams['figure.dpi'] = DPI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание датасета\n",
    "\n",
    "В данном разделе представлен первичный анализ загруженного датасета, включающий информацию о структуре данных, основных статистических характеристиках признаков, а также визуализацию распределения признаков и целевой переменной.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Форма данных:\", df.shape)\n",
    "print(\"\\nПервые строки:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Информация о данных\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Описательная статистика\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация распределения признаков и целевой переменной\n",
    "# Разделение на признаки и целевую переменную\n",
    "y = df['price'].copy()\n",
    "X = df.drop('price', axis=1).copy()\n",
    "\n",
    "# Определение типов признаков\n",
    "numeric_cols = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
    "binary_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
    "categorical_cols = ['furnishingstatus']\n",
    "\n",
    "# Кодирование бинарных признаков для визуализации\n",
    "X_viz = X.copy()\n",
    "for col in binary_cols:\n",
    "    if col in X_viz.columns:\n",
    "        X_viz[col] = X_viz[col].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# One-hot encoding для категориальной переменной\n",
    "if 'furnishingstatus' in X_viz.columns:\n",
    "    furnishing_dummies = pd.get_dummies(X_viz['furnishingstatus'], prefix='furnishing')\n",
    "    X_viz = pd.concat([X_viz.drop('furnishingstatus', axis=1), furnishing_dummies], axis=1)\n",
    "\n",
    "# Основные числовые признаки\n",
    "main_numeric_cols = [col for col in numeric_cols if col in X_viz.columns]\n",
    "furnishing_cols = [col for col in X_viz.columns if col.startswith('furnishing_')]\n",
    "all_cols = main_numeric_cols + furnishing_cols\n",
    "\n",
    "total_plots = len(all_cols) + 1\n",
    "n_cols = 4\n",
    "n_rows = (total_plots + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4*n_rows))\n",
    "if n_rows == 1:\n",
    "    axes = axes.reshape(1, -1) if hasattr(axes, 'reshape') else [axes]\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Распределение признаков\n",
    "for i, col in enumerate(all_cols):\n",
    "    if i < len(axes):\n",
    "        if col in furnishing_cols:\n",
    "            value_counts = X_viz[col].value_counts().sort_index()\n",
    "            axes[i].bar(value_counts.index.astype(str), value_counts.values, \n",
    "                       edgecolor='black', alpha=0.7, color='steelblue')\n",
    "        else:\n",
    "            axes[i].hist(X_viz[col], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "        \n",
    "        title = col.replace('_', ' ').title() if '_' in col else col.title()\n",
    "        axes[i].set_title(f'Распределение {title}', fontsize=10)\n",
    "        axes[i].set_xlabel(title, fontsize=8)\n",
    "        axes[i].set_ylabel('Частота', fontsize=8)\n",
    "        axes[i].tick_params(labelsize=8)\n",
    "        axes[i].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Распределение целевой переменной\n",
    "target_idx = len(all_cols)\n",
    "if target_idx < len(axes):\n",
    "    axes[target_idx].hist(y, bins=30, edgecolor='black', color='green', alpha=0.7)\n",
    "    axes[target_idx].set_title('Распределение целевой переменной\\n(Price)', fontsize=10)\n",
    "    axes[target_idx].set_xlabel('Цена (Price)', fontsize=8)\n",
    "    axes[target_idx].set_ylabel('Частота', fontsize=8)\n",
    "    axes[target_idx].tick_params(labelsize=8)\n",
    "    axes[target_idx].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Скрыть лишние subplot'ы\n",
    "for i in range(target_idx + 1, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "photos_path = os.path.join(photos_dir, 'distributions.png')\n",
    "plt.savefig(photos_path, dpi=DPI, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение матрицы корреляций\n",
    "X_with_target = X_viz.copy()\n",
    "X_with_target['price'] = y\n",
    "\n",
    "corr_matrix = X_with_target.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Матрица корреляций', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "photos_path = os.path.join(photos_dir, 'correlation_matrix.png')\n",
    "plt.savefig(photos_path, dpi=DPI, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "corr_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных\n",
    "\n",
    "В данном разделе подробно изложены процедуры предобработки данных, включающие удаление пропущенных значений, кодирование категориальных переменных, нормализацию данных и разделение выборки на обучающую и тестовую части.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобработка данных\n",
    "# Удаление пропусков\n",
    "df_processed = df.dropna()\n",
    "print(f\"Исходный размер данных: {len(df)}\")\n",
    "print(f\"Размер данных после удаления пропусков: {len(df_processed)}\")\n",
    "\n",
    "# Разделение на признаки и целевую переменную\n",
    "y = df_processed['price'].copy()\n",
    "X = df_processed.drop('price', axis=1).copy()\n",
    "\n",
    "print(f\"\\nИсходные признаки: {list(X.columns)}\")\n",
    "print(f\"Целевая переменная: price\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодирование бинарных признаков (yes/no -> 1/0)\n",
    "binary_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
    "for col in binary_cols:\n",
    "    if col in X.columns:\n",
    "        X[col] = X[col].map({'yes': 1, 'no': 0})\n",
    "        print(f\"Закодирован признак {col}: yes -> 1, no -> 0\")\n",
    "\n",
    "# One-hot encoding для категориальной переменной\n",
    "if 'furnishingstatus' in X.columns:\n",
    "    furnishing_dummies = pd.get_dummies(X['furnishingstatus'], prefix='furnishing')\n",
    "    X = pd.concat([X.drop('furnishingstatus', axis=1), furnishing_dummies], axis=1)\n",
    "    print(f\"\\nПрименен One-Hot Encoding для признака furnishingstatus\")\n",
    "    print(f\"Созданы признаки: {list(furnishing_dummies.columns)}\")\n",
    "\n",
    "print(f\"\\nПризнаки после кодирования: {list(X.columns)}\")\n",
    "print(f\"Количество признаков: {X.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка типов данных\n",
    "print(\"Типы данных:\")\n",
    "print(X.dtypes)\n",
    "\n",
    "print(\"\\nОписательная статистика после предобработки:\")\n",
    "X.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчет VIF-коэффициентов для выявления мультиколлинеарности\n",
    "# Удаляем один из one-hot encoded признаков для избежания мультиколлинеарности\n",
    "X_for_vif = X.copy()\n",
    "furnishing_cols = [col for col in X_for_vif.columns if col.startswith('furnishing_')]\n",
    "if len(furnishing_cols) > 0:\n",
    "    X_for_vif = X_for_vif.drop(furnishing_cols[-1], axis=1)\n",
    "    print(f\"Удален признак {furnishing_cols[-1]} для расчета VIF (избежание мультиколлинеарности one-hot признаков)\")\n",
    "\n",
    "# Убеждаемся, что все данные числовые\n",
    "X_for_vif = X_for_vif.select_dtypes(include=[np.number])\n",
    "X_for_vif = X_for_vif.replace([np.inf, -np.inf], np.nan)\n",
    "X_for_vif = X_for_vif.fillna(X_for_vif.mean())\n",
    "\n",
    "# Преобразуем в numpy array для VIF\n",
    "X_vif_array = X_for_vif.values.astype(float)\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Признак\"] = X_for_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif_array, i) for i in range(X_vif_array.shape[1])]\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "print(\"\\nVIF коэффициенты:\")\n",
    "print(vif_data.to_string(index=False))\n",
    "\n",
    "# Вывод признаков с высокой мультиколлинеарностью\n",
    "high_vif = vif_data[vif_data['VIF'] > 10]\n",
    "if len(high_vif) > 0:\n",
    "    print(f\"\\nПризнаки с высокой мультиколлинеарностью (VIF > 10):\")\n",
    "    print(high_vif.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nМультиколлинеарность не обнаружена (все VIF < 10)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение выборки на обучающую и тестовую (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Размер обучающей выборки: {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Размер тестовой выборки: {X_test.shape[0]} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Количество признаков: {X_train.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ход работы\n",
    "\n",
    "### Регрессионные модели до устранения мультиколлинеарности\n",
    "\n",
    "В данном разделе подробно описаны выбранные варианты реализации регрессии до устранения мультиколлинеарности. Для решения задачи предсказания цены недвижимости были выбраны два метода регрессионного анализа: линейная регрессия и гребневая регрессия (Ridge Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для расчета метрик качества\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Расчет метрик качества\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    return {'RMSE': rmse, 'R2': r2, 'MAPE': mape}\n",
    "\n",
    "# Функция для обучения моделей\n",
    "def train_models(X_train, X_test, y_train, y_test, scaler=None):\n",
    "    \"\"\"Обучение моделей линейной и гребневой регрессии\"\"\"\n",
    "    if scaler:\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_scaled = X_train\n",
    "        X_test_scaled = X_test\n",
    "    \n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(alpha=RIDGE_ALPHA)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Обучение\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Предсказания\n",
    "        y_train_pred = model.predict(X_train_scaled)\n",
    "        y_test_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Метрики\n",
    "        train_metrics = calculate_metrics(y_train, y_train_pred)\n",
    "        test_metrics = calculate_metrics(y_test, y_test_pred)\n",
    "        \n",
    "        # Кросс-валидация\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, \n",
    "                                   cv=CV_FOLDS, scoring='r2')\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'train_metrics': train_metrics,\n",
    "            'test_metrics': test_metrics,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Train - RMSE: {train_metrics['RMSE']:.4f}, R²: {train_metrics['R2']:.4f}, MAPE: {train_metrics['MAPE']:.2f}%\")\n",
    "        print(f\"  Test  - RMSE: {test_metrics['RMSE']:.4f}, R²: {test_metrics['R2']:.4f}, MAPE: {test_metrics['MAPE']:.2f}%\")\n",
    "        print(f\"  CV R²: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Обучение моделей на исходных данных\n",
    "print(\"=\"*60)\n",
    "print(\"МОДЕЛИ НА ИСХОДНЫХ ДАННЫХ\")\n",
    "print(\"=\"*60)\n",
    "scaler_original = StandardScaler()\n",
    "results_original = train_models(X_train, X_test, y_train, y_test, scaler_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Представление метрик в табличном виде\n",
    "metrics_table_original = pd.DataFrame({\n",
    "    'Модель': [],\n",
    "    'Выборка': [],\n",
    "    'RMSE': [],\n",
    "    'R²': [],\n",
    "    'MAPE (%)': []\n",
    "})\n",
    "\n",
    "for name in results_original.keys():\n",
    "    metrics_table_original = pd.concat([metrics_table_original, pd.DataFrame({\n",
    "        'Модель': [name],\n",
    "        'Выборка': ['Train'],\n",
    "        'RMSE': [results_original[name]['train_metrics']['RMSE']],\n",
    "        'R²': [results_original[name]['train_metrics']['R2']],\n",
    "        'MAPE (%)': [results_original[name]['train_metrics']['MAPE']]\n",
    "    })], ignore_index=True)\n",
    "    \n",
    "    metrics_table_original = pd.concat([metrics_table_original, pd.DataFrame({\n",
    "        'Модель': [name],\n",
    "        'Выборка': ['Test'],\n",
    "        'RMSE': [results_original[name]['test_metrics']['RMSE']],\n",
    "        'R²': [results_original[name]['test_metrics']['R2']],\n",
    "        'MAPE (%)': [results_original[name]['test_metrics']['MAPE']]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "print(\"Метрики качества моделей на исходных данных:\")\n",
    "metrics_table_original\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Устранение мультиколлинеарности с помощью PCA\n",
    "\n",
    "Для устранения мультиколлинеарности и снижения размерности признаков был применен метод главных компонент (Principal Component Analysis, PCA). PCA представляет собой метод снижения размерности, который преобразует исходные признаки в новый набор ортогональных признаков, называемых главными компонентами. Главные компоненты упорядочены по убыванию объясненной дисперсии, то есть первая главная компонента объясняет наибольшую долю дисперсии данных, вторая - следующую наибольшую долю и так далее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применение PCA для снижения размерности\n",
    "# Стандартизация данных перед PCA\n",
    "scaler_pca = StandardScaler()\n",
    "X_train_scaled = scaler_pca.fit_transform(X_train)\n",
    "X_test_scaled = scaler_pca.transform(X_test)\n",
    "\n",
    "# Определение количества компонент\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_train_scaled)\n",
    "\n",
    "cumsum_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "n_components = np.argmax(cumsum_variance >= PCA_VARIANCE_THRESHOLD) + 1\n",
    "\n",
    "print(f\"Количество компонент для {PCA_VARIANCE_THRESHOLD*100}% дисперсии: {n_components}\")\n",
    "print(f\"Исходное количество признаков: {X_train.shape[1]}\")\n",
    "print(f\"Снижение размерности: {X_train.shape[1] - n_components} признаков ({((X_train.shape[1] - n_components)/X_train.shape[1]*100):.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График каменистой осыпи\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Накопленная объясненная дисперсия\n",
    "ax1.plot(range(1, len(cumsum_variance) + 1), cumsum_variance, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.axhline(y=PCA_VARIANCE_THRESHOLD, color='r', linestyle='--', linewidth=2,\n",
    "            label=f'{PCA_VARIANCE_THRESHOLD*100}% дисперсии')\n",
    "ax1.axvline(x=n_components, color='g', linestyle='--', linewidth=2,\n",
    "            label=f'Выбрано компонент: {n_components}')\n",
    "ax1.set_xlabel('Количество компонент', fontsize=12)\n",
    "ax1.set_ylabel('Накопленная объясненная дисперсия', fontsize=12)\n",
    "ax1.set_title('Накопленная объясненная дисперсия (PCA)', fontsize=14)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Индивидуальная объясненная дисперсия\n",
    "individual_variance = pca_full.explained_variance_ratio_\n",
    "ax2.bar(range(1, len(individual_variance) + 1), individual_variance, alpha=0.7, color='steelblue')\n",
    "ax2.set_xlabel('Номер компоненты', fontsize=12)\n",
    "ax2.set_ylabel('Объясненная дисперсия', fontsize=12)\n",
    "ax2.set_title('Индивидуальная объясненная дисперсия', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "photos_path = os.path.join(photos_dir, 'pca_scree.png')\n",
    "plt.savefig(photos_path, dpi=DPI, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применение PCA с выбранным количеством компонент\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"Объясненная дисперсия: {pca.explained_variance_ratio_.sum():.4f} ({pca.explained_variance_ratio_.sum()*100:.2f}%)\")\n",
    "print(f\"\\nРазмерность данных после PCA:\")\n",
    "print(f\"  Обучающая выборка: {X_train_pca.shape}\")\n",
    "print(f\"  Тестовая выборка: {X_test_pca.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регрессионные модели после применения PCA\n",
    "\n",
    "После применения PCA и получения главных компонент были заново построены модели линейной и гребневой регрессии, но теперь в качестве признаков использовались не исходные данные, а главные компоненты. Это позволило устранить проблему мультиколлинеарности, так как главные компоненты по определению являются ортогональными (некоррелированными) друг с другом.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение моделей на PCA компонентах\n",
    "print(\"=\"*60)\n",
    "print(\"МОДЕЛИ НА PCA КОМПОНЕНТАХ\")\n",
    "print(\"=\"*60)\n",
    "results_pca = train_models(X_train_pca, X_test_pca, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Представление метрик в табличном виде\n",
    "metrics_table_pca = pd.DataFrame({\n",
    "    'Модель': [],\n",
    "    'Выборка': [],\n",
    "    'RMSE': [],\n",
    "    'R²': [],\n",
    "    'MAPE (%)': []\n",
    "})\n",
    "\n",
    "for name in results_pca.keys():\n",
    "    metrics_table_pca = pd.concat([metrics_table_pca, pd.DataFrame({\n",
    "        'Модель': [name],\n",
    "        'Выборка': ['Train'],\n",
    "        'RMSE': [results_pca[name]['train_metrics']['RMSE']],\n",
    "        'R²': [results_pca[name]['train_metrics']['R2']],\n",
    "        'MAPE (%)': [results_pca[name]['train_metrics']['MAPE']]\n",
    "    })], ignore_index=True)\n",
    "    \n",
    "    metrics_table_pca = pd.concat([metrics_table_pca, pd.DataFrame({\n",
    "        'Модель': [name],\n",
    "        'Выборка': ['Test'],\n",
    "        'RMSE': [results_pca[name]['test_metrics']['RMSE']],\n",
    "        'R²': [results_pca[name]['test_metrics']['R2']],\n",
    "        'MAPE (%)': [results_pca[name]['test_metrics']['MAPE']]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "print(\"Метрики качества моделей на PCA компонентах:\")\n",
    "metrics_table_pca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение результатов\n",
    "\n",
    "В данном разделе представлено сравнение метрик качества моделей, обученных на исходных данных и на главных компонентах. Сравнение позволяет оценить влияние применения PCA на качество моделей и сделать выводы о целесообразности использования метода главных компонент для данной задачи.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение результатов моделей\n",
    "comparison = pd.DataFrame({\n",
    "    'Модель': [],\n",
    "    'Данные': [],\n",
    "    'RMSE': [],\n",
    "    'R²': [],\n",
    "    'MAPE (%)': []\n",
    "})\n",
    "\n",
    "for name in results_original.keys():\n",
    "    comparison = pd.concat([comparison, pd.DataFrame({\n",
    "        'Модель': [name],\n",
    "        'Данные': ['Исходные'],\n",
    "        'RMSE': [results_original[name]['test_metrics']['RMSE']],\n",
    "        'R²': [results_original[name]['test_metrics']['R2']],\n",
    "        'MAPE (%)': [results_original[name]['test_metrics']['MAPE']]\n",
    "    })], ignore_index=True)\n",
    "    \n",
    "    comparison = pd.concat([comparison, pd.DataFrame({\n",
    "        'Модель': [name],\n",
    "        'Данные': ['PCA'],\n",
    "        'RMSE': [results_pca[name]['test_metrics']['RMSE']],\n",
    "        'R²': [results_pca[name]['test_metrics']['R2']],\n",
    "        'MAPE (%)': [results_pca[name]['test_metrics']['MAPE']]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"СРАВНЕНИЕ РЕЗУЛЬТАТОВ\")\n",
    "print(\"=\"*60)\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация сравнения метрик\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics_to_plot = ['RMSE', 'R²', 'MAPE (%)']\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Подготовка данных для графика\n",
    "    models = comparison['Модель'].unique()\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    original_values = [comparison[(comparison['Модель'] == model) & (comparison['Данные'] == 'Исходные')][metric].values[0] for model in models]\n",
    "    pca_values = [comparison[(comparison['Модель'] == model) & (comparison['Данные'] == 'PCA')][metric].values[0] for model in models]\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, original_values, width, label='Исходные данные', alpha=0.8, color='steelblue')\n",
    "    bars2 = ax.bar(x + width/2, pca_values, width, label='PCA', alpha=0.8, color='coral')\n",
    "    \n",
    "    ax.set_xlabel('Модель', fontsize=12)\n",
    "    ax.set_ylabel(metric, fontsize=12)\n",
    "    ax.set_title(f'Сравнение {metric}', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models, rotation=15, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Добавление значений на столбцы\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.4f}' if metric != 'MAPE (%)' else f'{height:.2f}%',\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "photos_path = os.path.join(photos_dir, 'metrics_comparison.png')\n",
    "plt.savefig(photos_path, dpi=DPI, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "\n",
    "В ходе выполнения лабораторной работы была проведена полная процедура обучения моделей линейной регрессии на реальном датасете недвижимости. Были реализованы и протестированы два метода регрессионного анализа: линейная регрессия и гребневая регрессия. Проведен анализ мультиколлинеарности признаков с помощью расчета VIF-коэффициентов, применен метод главных компонент для устранения мультиколлинеарности и снижения размерности признаков.\n",
    "\n",
    "Результаты работы показали, что обе модели (линейная и гребневая регрессия) демонстрируют сопоставимое качество на исходных данных. Применение PCA позволило значительно снизить размерность пространства признаков при сохранении высокой доли объясненной дисперсии (95%). Сравнение метрик качества моделей, обученных на исходных данных и на главных компонентах, показало влияние применения PCA на результаты моделирования.\n",
    "\n",
    "Метод главных компонент эффективно устраняет мультиколлинеарность и позволяет работать с меньшим количеством признаков, что может улучшить обобщающую способность модели и ускорить процесс обучения. Однако применение PCA может привести к потере интерпретируемости модели, так как главные компоненты являются линейными комбинациями исходных признаков и не имеют прямой физической интерпретации.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Список источников\n",
    "\n",
    "1. Scikit-learn: Machine Learning in Python. URL: https://scikit-learn.org/stable/\n",
    "\n",
    "2. Pandas: Python Data Analysis Library. URL: https://pandas.pydata.org/\n",
    "\n",
    "3. NumPy: The fundamental package for scientific computing with Python. URL: https://numpy.org/\n",
    "\n",
    "4. Matplotlib: Visualization with Python. URL: https://matplotlib.org/\n",
    "\n",
    "5. Seaborn: Statistical data visualization. URL: https://seaborn.pydata.org/\n",
    "\n",
    "6. Statsmodels: Econometric and statistical modeling with Python. URL: https://www.statsmodels.org/\n",
    "\n",
    "7. James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Springer.\n",
    "\n",
    "8. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.\n",
    "\n",
    "9. Jolliffe, I. T., & Cadima, J. (2016). Principal component analysis: a review and recent developments. Philosophical Transactions of the Royal Society A, 374(2065), 20150202.\n",
    "\n",
    "10. Kaggle: Your Machine Learning and Data Science Community. URL: https://www.kaggle.com/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
