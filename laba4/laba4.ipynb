{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Лабораторная работа 4: Кластеризация\n",
        "\n",
        "## Введение\n",
        "\n",
        "Целью данной лабораторной работы является изучение алгоритмов кластеризации, приобретение навыков оценки качества разбиения данных на кластеры и интерпретации результатов. Кластеризация представляет собой задачу обучения без учителя, направленную на группировку объектов в кластеры таким образом, чтобы объекты внутри одного кластера были более похожи друг на друга, чем на объекты из других кластеров.\n",
        "\n",
        "В рамках работы необходимо загрузить датасет для задачи кластеризации, провести дескриптивный анализ данных, выполнить стандартизацию признаков, реализовать кластеризацию двумя различными методами и оценить качество полученных разбиений с помощью внутренних и внешних метрик. Дополнительно необходимо исследовать влияние параметров методов на качество кластеризации и визуализировать полученные результаты.\n",
        "\n",
        "Работа направлена на приобретение практических навыков работы с алгоритмами кластеризации, понимание их принципов работы, умение выбирать оптимальные параметры методов и интерпретировать результаты кластеризации. Особое внимание уделяется методам определения оптимального количества кластеров, таким как метод локтя и силуэтный анализ.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Импорт необходимых библиотек\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (silhouette_score, calinski_harabasz_score, \n",
        "                            davies_bouldin_score, adjusted_rand_score,\n",
        "                            normalized_mutual_info_score, homogeneity_score,\n",
        "                            completeness_score, v_measure_score)\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Настройки\n",
        "DATA_PATH = 'Iris.csv'\n",
        "RANDOM_STATE = 42\n",
        "K_RANGE = range(2, 11)\n",
        "N_INIT = 10\n",
        "MAX_ITER = 300\n",
        "PCA_COMPONENTS = 2\n",
        "FIG_SIZE = (12, 8)\n",
        "DPI = 100\n",
        "USE_KMEANS = True\n",
        "USE_HIERARCHICAL = True\n",
        "USE_DBSCAN = False\n",
        "USE_GAUSSIAN_MIXTURE = False\n",
        "\n",
        "# Создание папки для графиков\n",
        "photos_dir = os.path.join('photos')\n",
        "os.makedirs(photos_dir, exist_ok=True)\n",
        "\n",
        "# Настройка визуализации\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = FIG_SIZE\n",
        "plt.rcParams['figure.dpi'] = DPI\n",
        "\n",
        "# Фиксация seed для воспроизводимости\n",
        "np.random.seed(RANDOM_STATE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Описание датасета\n",
        "\n",
        "В данной работе используется датасет Iris, который является классическим набором данных для задач машинного обучения. Датасет содержит информацию о трех видах цветов ириса и включает четыре признака: длина чашелистика, ширина чашелистика, длина лепестка и ширина лепестка. Хотя в датасете присутствуют метки классов, они используются только для оценки качества кластеризации с помощью внешних метрик, а не для обучения алгоритмов кластеризации.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Загрузка и анализ данных\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Форма данных:\", df.shape)\n",
        "print(\"\\nПервые строки:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Информация о данных\n",
        "df.info()\n",
        "\n",
        "print(\"\\nОписательная статистика:\")\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Визуализация распределения признаков\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "feature_names = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
        "for i, col in enumerate(feature_names):\n",
        "    axes[i].hist(df[col], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "    axes[i].set_title(f'Распределение {col}', fontsize=12)\n",
        "    axes[i].set_xlabel(col, fontsize=10)\n",
        "    axes[i].set_ylabel('Частота', fontsize=10)\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "photos_path = os.path.join(photos_dir, 'distributions.png')\n",
        "plt.savefig(photos_path, dpi=DPI, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Матрица диаграмм рассеивания\n",
        "if 'Species' in df.columns:\n",
        "    sns.pairplot(df, hue='Species', diag_kind='hist')\n",
        "    plt.suptitle('Матрица диаграмм рассеивания', y=1.02, fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    photos_path = os.path.join(photos_dir, 'pairplot.png')\n",
        "    plt.savefig(photos_path, dpi=DPI, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Предобработка данных\n",
        "\n",
        "Перед применением алгоритмов кластеризации была проведена стандартизация данных с помощью StandardScaler, которая преобразует признаки таким образом, чтобы они имели среднее значение, равное нулю, и стандартное отклонение, равное единице. Стандартизация является критически важным этапом предобработки для алгоритмов кластеризации, так как признаки с большими значениями могут доминировать в расчете расстояний между объектами, что приведет к некорректным результатам кластеризации.\n",
        "\n",
        "Выбор метода масштабирования обоснован тем, что стандартизация сохраняет информацию о распределении признаков и делает их сопоставимыми по масштабу, что особенно важно для методов, основанных на расчете расстояний, таких как K-means и иерархическая кластеризация.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Предобработка данных\n",
        "# Удаление ID если есть\n",
        "if 'Id' in df.columns:\n",
        "    df = df.drop('Id', axis=1)\n",
        "\n",
        "# Сохранение меток классов если есть (для внешних метрик)\n",
        "y_true = None\n",
        "if 'Species' in df.columns:\n",
        "    y_true = df['Species'].copy()\n",
        "    df_features = df.drop('Species', axis=1)\n",
        "else:\n",
        "    df_features = df.copy()\n",
        "\n",
        "# Стандартизация\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df_features)\n",
        "\n",
        "print(f\"Признаки: {list(df_features.columns)}\")\n",
        "print(f\"Размерность после стандартизации: {X_scaled.shape}\")\n",
        "if y_true is not None:\n",
        "    print(f\"Классы: {y_true.unique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ход работы\n",
        "\n",
        "### Подбор оптимального количества кластеров\n",
        "\n",
        "Определение оптимального количества кластеров является одной из наиболее важных задач в кластеризации, так как большинство алгоритмов требуют указания этого параметра заранее. В данной работе использовались два метода для определения оптимального количества кластеров: метод локтя и силуэтный анализ.\n",
        "\n",
        "Метод локтя основан на анализе зависимости суммы квадратов расстояний от объектов до центров их кластеров (инерции) от количества кластеров. При увеличении количества кластеров инерция уменьшается, и в некоторой точке скорость уменьшения резко замедляется, образуя \"локоть\" на графике. Эта точка обычно соответствует оптимальному количеству кластеров.\n",
        "\n",
        "Силуэтный анализ оценивает качество кластеризации, вычисляя для каждого объекта коэффициент силуэта, который показывает, насколько хорошо объект соответствует своему кластеру по сравнению с другими кластерами. Коэффициент силуэта принимает значения от -1 до 1, где значения близкие к 1 указывают на хорошее соответствие объекта своему кластеру. Оптимальное количество кластеров соответствует максимальному среднему коэффициенту силуэта.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Метод локтя для определения оптимального k\n",
        "print(\"=\"*60)\n",
        "print(\"ПОДБОР ОПТИМАЛЬНОГО КОЛИЧЕСТВА КЛАСТЕРОВ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "inertias = []\n",
        "k_range = list(K_RANGE)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, n_init=N_INIT, \n",
        "                   max_iter=MAX_ITER, random_state=RANDOM_STATE)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
        "plt.xlabel('Количество кластеров (k)', fontsize=12)\n",
        "plt.ylabel('Инерция (Inertia)', fontsize=12)\n",
        "plt.title('Метод локтя для определения оптимального k', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "photos_path = os.path.join(photos_dir, 'elbow_method.png')\n",
        "plt.savefig(photos_path, dpi=DPI, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Силуэтный анализ для определения оптимального k\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, n_init=N_INIT,\n",
        "                   max_iter=MAX_ITER, random_state=RANDOM_STATE)\n",
        "    labels = kmeans.fit_predict(X_scaled)\n",
        "    score = silhouette_score(X_scaled, labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
        "plt.axvline(x=optimal_k, color='green', linestyle='--', linewidth=2,\n",
        "           label=f'Оптимальное k = {optimal_k}')\n",
        "plt.xlabel('Количество кластеров (k)', fontsize=12)\n",
        "plt.ylabel('Силуэтный коэффициент', fontsize=12)\n",
        "plt.title('Силуэтный анализ для определения оптимального k', fontsize=14)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "photos_path = os.path.join(photos_dir, 'silhouette_analysis.png')\n",
        "plt.savefig(photos_path, dpi=DPI, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nОптимальное k по силуэтному анализу: {optimal_k}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Кластеризация методом K-means\n",
        "\n",
        "Метод K-means является одним из наиболее популярных алгоритмов кластеризации благодаря своей простоте и эффективности. Алгоритм работает путем итеративного обновления центров кластеров и назначения объектов ближайшим центрам. Процесс продолжается до тех пор, пока центры кластеров не перестанут изменяться или не будет достигнуто максимальное количество итераций.\n",
        "\n",
        "Основные преимущества K-means включают простоту реализации, эффективность для больших наборов данных и способность находить кластеры сферической формы. Однако метод имеет ограничения: он требует указания количества кластеров заранее, чувствителен к начальной инициализации центров и может плохо работать с кластерами неправильной формы или различной плотности.\n",
        "\n",
        "В данной работе для повышения устойчивости результатов использовалось несколько случайных инициализаций (n_init=10), что позволяет избежать попадания в локальные минимумы и получить более надежные результаты кластеризации.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Кластеризация методом K-means\n",
        "if USE_KMEANS:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"КЛАСТЕРИЗАЦИЯ K-MEANS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    kmeans = KMeans(n_clusters=optimal_k, n_init=N_INIT,\n",
        "                   max_iter=MAX_ITER, random_state=RANDOM_STATE)\n",
        "    labels_kmeans = kmeans.fit_predict(X_scaled)\n",
        "    centers = kmeans.cluster_centers_\n",
        "    \n",
        "    print(f\"Количество кластеров: {optimal_k}\")\n",
        "    print(f\"Инерция: {kmeans.inertia_:.4f}\")\n",
        "    print(f\"Количество итераций: {kmeans.n_iter_}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Иерархическая кластеризация\n",
        "\n",
        "Иерархическая кластеризация представляет собой метод, который строит иерархию кластеров в виде дерева, называемого дендрограммой. Алгоритм может работать двумя способами: агломеративно (снизу вверх), начиная с отдельных объектов и последовательно объединяя их в кластеры, или дивизивно (сверху вниз), начиная с одного кластера и последовательно разделяя его.\n",
        "\n",
        "В данной работе использовался агломеративный подход с методом связи Ward, который минимизирует дисперсию внутри кластеров при их объединении. Метод Ward является особенно эффективным для кластеров сферической формы и хорошо работает с данными, имеющими нормальное распределение.\n",
        "\n",
        "Дендрограмма позволяет визуально оценить структуру данных и определить оптимальное количество кластеров, анализируя высоту слияний кластеров. Большие расстояния между слияниями указывают на естественные границы между кластерами.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Иерархическая кластеризация\n",
        "if USE_HIERARCHICAL:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ИЕРАРХИЧЕСКАЯ КЛАСТЕРИЗАЦИЯ\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Построение дендрограммы\n",
        "    linkage_matrix = linkage(X_scaled, method='ward')\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    dendrogram(linkage_matrix, truncate_mode='level', p=10)\n",
        "    plt.title('Дендрограмма (linkage: ward)', fontsize=14)\n",
        "    plt.xlabel('Образцы', fontsize=12)\n",
        "    plt.ylabel('Расстояние', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    photos_path = os.path.join(photos_dir, 'dendrogram_ward.png')\n",
        "    plt.savefig(photos_path, dpi=DPI, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Кластеризация\n",
        "    hierarchical = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')\n",
        "    labels_hierarchical = hierarchical.fit_predict(X_scaled)\n",
        "    \n",
        "    print(f\"Количество кластеров: {optimal_k}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Расчет метрик качества кластеризации\n",
        "\n",
        "Для оценки качества кластеризации использовались как внутренние, так и внешние метрики. Внутренние метрики оценивают качество кластеризации на основе структуры данных без использования информации о реальных классах. К таким метрикам относятся силуэтный коэффициент, индекс Calinski-Harabasz и индекс Davies-Bouldin.\n",
        "\n",
        "Силуэтный коэффициент оценивает, насколько хорошо объекты соответствуют своим кластерам. Индекс Calinski-Harabasz измеряет отношение межкластерной дисперсии к внутрикластерной дисперсии, при этом более высокие значения указывают на лучшее качество кластеризации. Индекс Davies-Bouldin оценивает среднее сходство между кластерами, при этом более низкие значения указывают на лучшее разделение кластеров.\n",
        "\n",
        "Внешние метрики используются, когда известны истинные метки классов, и позволяют оценить, насколько хорошо результаты кластеризации соответствуют реальному разделению на классы. К таким метрикам относятся скорректированный индекс Rand, нормализованная взаимная информация, однородность, полнота и V-мера.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Функция для расчета метрик качества кластеризации\n",
        "def calculate_metrics(X, labels, y_true=None):\n",
        "    \"\"\"Расчет метрик качества кластеризации\"\"\"\n",
        "    metrics = {}\n",
        "    \n",
        "    # Внутренние метрики\n",
        "    if len(set(labels)) > 1 and -1 not in labels:\n",
        "        metrics['Silhouette Score'] = silhouette_score(X, labels)\n",
        "        metrics['Calinski-Harabasz Index'] = calinski_harabasz_score(X, labels)\n",
        "        metrics['Davies-Bouldin Index'] = davies_bouldin_score(X, labels)\n",
        "    else:\n",
        "        metrics['Silhouette Score'] = -1\n",
        "        metrics['Calinski-Harabasz Index'] = 0\n",
        "        metrics['Davies-Bouldin Index'] = float('inf')\n",
        "    \n",
        "    # Внешние метрики (если известны истинные классы)\n",
        "    if y_true is not None:\n",
        "        le = LabelEncoder()\n",
        "        y_encoded = le.fit_transform(y_true)\n",
        "        \n",
        "        metrics['Adjusted Rand Index'] = adjusted_rand_score(y_encoded, labels)\n",
        "        metrics['Normalized Mutual Info'] = normalized_mutual_info_score(y_encoded, labels)\n",
        "        metrics['Homogeneity'] = homogeneity_score(y_encoded, labels)\n",
        "        metrics['Completeness'] = completeness_score(y_encoded, labels)\n",
        "        metrics['V-measure'] = v_measure_score(y_encoded, labels)\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Расчет метрик для K-means\n",
        "if USE_KMEANS:\n",
        "    metrics_kmeans = calculate_metrics(X_scaled, labels_kmeans, y_true)\n",
        "    print(\"\\nМетрики K-means:\")\n",
        "    for metric, value in metrics_kmeans.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "# Расчет метрик для иерархической кластеризации\n",
        "if USE_HIERARCHICAL:\n",
        "    metrics_hierarchical = calculate_metrics(X_scaled, labels_hierarchical, y_true)\n",
        "    print(\"\\nМетрики Иерархическая кластеризация:\")\n",
        "    for metric, value in metrics_hierarchical.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Визуализация результатов кластеризации\n",
        "\n",
        "Для визуализации результатов кластеризации использовался метод главных компонент (PCA) для снижения размерности данных до двух измерений. Это позволяет визуализировать кластеры на плоскости, сохраняя при этом максимально возможную информацию о структуре данных. Каждая точка на графике представляет объект, а цвет точки соответствует кластеру, к которому он был отнесен.\n",
        "\n",
        "Для метода K-means также визуализируются центры кластеров, которые показывают характерные значения признаков для каждого кластера. Сравнение результатов кластеризации с истинными классами (если они известны) позволяет оценить соответствие найденных кластеров реальному разделению данных на классы.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Функция для визуализации кластеров\n",
        "def visualize_clusters(X, labels, centers=None, method_name='', y_true=None):\n",
        "    \"\"\"Визуализация кластеров с использованием PCA\"\"\"\n",
        "    pca = PCA(n_components=PCA_COMPONENTS)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Визуализация кластеров\n",
        "    scatter = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels, \n",
        "                              cmap='viridis', alpha=0.6, s=50)\n",
        "    if centers is not None:\n",
        "        centers_pca = pca.transform(centers)\n",
        "        axes[0].scatter(centers_pca[:, 0], centers_pca[:, 1], \n",
        "                       c='red', marker='x', s=200, linewidths=3, label='Центры')\n",
        "        axes[0].legend()\n",
        "    axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)', fontsize=12)\n",
        "    axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)', fontsize=12)\n",
        "    axes[0].set_title(f'Кластеры: {method_name}', fontsize=14)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    plt.colorbar(scatter, ax=axes[0])\n",
        "    \n",
        "    # Сравнение с истинными классами (если есть)\n",
        "    if y_true is not None:\n",
        "        le = LabelEncoder()\n",
        "        y_encoded = le.fit_transform(y_true)\n",
        "        scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y_encoded,\n",
        "                                  cmap='Set1', alpha=0.6, s=50)\n",
        "        axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)', fontsize=12)\n",
        "        axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)', fontsize=12)\n",
        "        axes[1].set_title('Истинные классы', fontsize=14)\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        plt.colorbar(scatter2, ax=axes[1])\n",
        "    else:\n",
        "        axes[1].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    photos_path = os.path.join(photos_dir, f'clusters_{method_name.lower().replace(\" \", \"_\")}.png')\n",
        "    plt.savefig(photos_path, dpi=DPI, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Визуализация результатов\n",
        "if USE_KMEANS:\n",
        "    visualize_clusters(X_scaled, labels_kmeans, centers, 'K-means', y_true)\n",
        "    \n",
        "    # Анализ центров кластеров\n",
        "    centers_df = pd.DataFrame(centers, columns=df_features.columns)\n",
        "    print(\"\\nЦентры кластеров:\")\n",
        "    print(centers_df)\n",
        "\n",
        "if USE_HIERARCHICAL:\n",
        "    visualize_clusters(X_scaled, labels_hierarchical, None, 'Hierarchical', y_true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Исследование влияния параметров на качество кластеризации\n",
        "\n",
        "Для понимания влияния параметров методов кластеризации на качество результатов был проведен эксперимент, в котором последовательно изменялось количество кластеров k для метода K-means, и для каждого значения анализировались метрики качества кластеризации. Результаты эксперимента показывают, как изменение параметра k влияет на силуэтный коэффициент и индекс Calinski-Harabasz.\n",
        "\n",
        "Анализ влияния параметров позволяет выбрать оптимальные значения для конкретного набора данных и понять, насколько устойчивы результаты кластеризации к изменению параметров. Это особенно важно для практического применения методов кластеризации, когда необходимо найти баланс между качеством кластеризации и интерпретируемостью результатов.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Исследование влияния параметров на качество кластеризации\n",
        "if USE_KMEANS:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ЭКСПЕРИМЕНТЫ С ПАРАМЕТРАМИ: K-MEANS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    k_range = list(K_RANGE)\n",
        "    silhouette_scores_exp = []\n",
        "    ch_scores = []\n",
        "    \n",
        "    for k in k_range:\n",
        "        kmeans = KMeans(n_clusters=k, n_init=N_INIT,\n",
        "                       max_iter=MAX_ITER, random_state=RANDOM_STATE)\n",
        "        labels = kmeans.fit_predict(X_scaled)\n",
        "        if len(set(labels)) > 1:\n",
        "            silhouette_scores_exp.append(silhouette_score(X_scaled, labels))\n",
        "            ch_scores.append(calinski_harabasz_score(X_scaled, labels))\n",
        "        else:\n",
        "            silhouette_scores_exp.append(-1)\n",
        "            ch_scores.append(0)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    axes[0].plot(k_range, silhouette_scores_exp, 'o-', linewidth=2, markersize=8)\n",
        "    axes[0].set_xlabel('Количество кластеров (k)', fontsize=12)\n",
        "    axes[0].set_ylabel('Силуэтный коэффициент', fontsize=12)\n",
        "    axes[0].set_title('Влияние k на силуэтный коэффициент', fontsize=14)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    axes[1].plot(k_range, ch_scores, 'o-', color='orange', linewidth=2, markersize=8)\n",
        "    axes[1].set_xlabel('Количество кластеров (k)', fontsize=12)\n",
        "    axes[1].set_ylabel('Calinski-Harabasz Index', fontsize=12)\n",
        "    axes[1].set_title('Влияние k на Calinski-Harabasz Index', fontsize=14)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    photos_path = os.path.join(photos_dir, 'parameter_experiments_kmeans.png')\n",
        "    plt.savefig(photos_path, dpi=DPI, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Сравнение результатов\n",
        "\n",
        "Для проведения сравнительного анализа методов кластеризации были рассчитаны метрики качества для каждого метода и представлены в сравнительной таблице. Сравнение позволяет оценить относительную эффективность различных методов кластеризации на данном наборе данных и сделать выводы о применимости методов к данному типу данных.\n",
        "\n",
        "Анализ метрик показывает, насколько хорошо каждый метод разделяет данные на кластеры, насколько компактны полученные кластеры и насколько хорошо результаты кластеризации соответствуют истинному разделению на классы (если оно известно). Это позволяет выбрать наиболее подходящий метод для конкретной задачи и понять его преимущества и ограничения.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Сравнение результатов\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"СРАВНЕНИЕ РЕЗУЛЬТАТОВ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "comparison = pd.DataFrame()\n",
        "if USE_KMEANS:\n",
        "    comparison = pd.concat([comparison, pd.DataFrame({\n",
        "        'Метод': ['K-means'],\n",
        "        **{k: [v] for k, v in metrics_kmeans.items()}\n",
        "    })], ignore_index=True)\n",
        "\n",
        "if USE_HIERARCHICAL:\n",
        "    comparison = pd.concat([comparison, pd.DataFrame({\n",
        "        'Метод': ['Hierarchical'],\n",
        "        **{k: [v] for k, v in metrics_hierarchical.items()}\n",
        "    })], ignore_index=True)\n",
        "\n",
        "if comparison.shape[0] > 0:\n",
        "    print(\"\\nСравнительная таблица метрик:\")\n",
        "    print(comparison.to_string(index=False))\n",
        "    \n",
        "    # Визуализация сравнения\n",
        "    if len(comparison) > 1:\n",
        "        metrics_to_plot = [col for col in comparison.columns if col != 'Метод']\n",
        "        fig, axes = plt.subplots(1, len(metrics_to_plot), figsize=(5*len(metrics_to_plot), 6))\n",
        "        if len(metrics_to_plot) == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for idx, metric in enumerate(metrics_to_plot):\n",
        "            axes[idx].bar(comparison['Метод'], comparison[metric], color=['steelblue', 'orange'])\n",
        "            axes[idx].set_ylabel(metric, fontsize=12)\n",
        "            axes[idx].set_title(f'Сравнение {metric}', fontsize=14)\n",
        "            axes[idx].grid(True, alpha=0.3, axis='y')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        photos_path = os.path.join(photos_dir, 'comparison.png')\n",
        "        plt.savefig(photos_path, dpi=DPI, bbox_inches='tight')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Заключение\n",
        "\n",
        "В ходе выполнения лабораторной работы был проведен полный анализ алгоритмов кластеризации на датасете Iris. Были реализованы и протестированы два метода кластеризации: K-means и иерархическая кластеризация. Проведен анализ методов определения оптимального количества кластеров, рассчитаны метрики качества кластеризации и исследовано влияние параметров на результаты.\n",
        "\n",
        "Результаты работы показали, что оба метода кластеризации успешно разделяют данные на кластеры, причем результаты в значительной степени соответствуют истинному разделению на классы. Метод локтя и силуэтный анализ дали согласованные результаты относительно оптимального количества кластеров, что подтверждает корректность выбора параметров.\n",
        "\n",
        "Основные выводы работы заключаются в том, что стандартизация данных является критически важным этапом предобработки для алгоритмов кластеризации, основанных на расчете расстояний. Оба рассмотренных метода показали хорошие результаты на данном датасете, что объясняется хорошо разделимой структурой данных. Иерархическая кластеризация предоставляет дополнительную информацию о структуре данных через дендрограмму, в то время как K-means является более эффективным для больших наборов данных.\n",
        "\n",
        "Полученные результаты могут быть использованы для выбора оптимального метода кластеризации для подобных задач и понимания влияния различных параметров на качество кластеризации. Интерпретация центров кластеров позволяет понять характерные особенности выделенных групп объектов, что важно для практического применения методов кластеризации.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Список источников\n",
        "\n",
        "1. Scikit-learn: Machine Learning in Python. URL: https://scikit-learn.org/stable/\n",
        "\n",
        "2. Pandas: Python Data Analysis Library. URL: https://pandas.pydata.org/\n",
        "\n",
        "3. NumPy: The fundamental package for scientific computing with Python. URL: https://numpy.org/\n",
        "\n",
        "4. Matplotlib: Visualization with Python. URL: https://matplotlib.org/\n",
        "\n",
        "5. Seaborn: Statistical data visualization. URL: https://seaborn.pydata.org/\n",
        "\n",
        "6. SciPy: Scientific Computing Library for Python. URL: https://scipy.org/\n",
        "\n",
        "7. MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, 1(14), 281-297.\n",
        "\n",
        "8. Ward, J. H. (1963). Hierarchical grouping to optimize an objective function. Journal of the American statistical association, 58(301), 236-244.\n",
        "\n",
        "9. Rousseeuw, P. J. (1987). Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics, 20, 53-65.\n",
        "\n",
        "10. Kaggle: Your Machine Learning and Data Science Community. URL: https://www.kaggle.com/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Приложение\n",
        "\n",
        "Полный листинг программного кода представлен в ячейках данного Jupyter Notebook. Все функции и процедуры, использованные в работе, реализованы с использованием стандартных библиотек Python для машинного обучения и анализа данных.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
